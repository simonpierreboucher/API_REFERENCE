{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# OPENAI, ANTHROPIC, MISTRAL, COHERE and VOYAGE (Chat completion and embedding) Reference\n",
        "\n",
        "## M-LAI\n"
      ],
      "metadata": {
        "id": "e5YMeU0cRvsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANTHROPIC Chat Completion\n",
        "\n",
        "The **Messages API** facilitates conversational interactions by generating model responses based on structured input messages. It supports both single-turn and multi-turn stateless conversations.\n",
        "\n",
        "---\n",
        "\n",
        "## Endpoint\n",
        "```\n",
        "POST https://api.anthropic.com/v1/messages\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Headers\n",
        "\n",
        "| Header               | Type    | Required | Description                                                                 |\n",
        "|----------------------|---------|----------|-----------------------------------------------------------------------------|\n",
        "| **x-api-key**        | string  | Required | Your unique API key for authentication. Obtain this key from the console.  |\n",
        "| **anthropic-version**| string  | Required | The API version to use (e.g., `2023-06-01`).                               |\n",
        "| **content-type**     | string  | Required | Must be `application/json`.                                                |\n",
        "| **anthropic-beta**   | string[]| Optional | Specify one or more beta versions to use (e.g., `beta1,beta2`).            |\n",
        "\n",
        "---\n",
        "\n",
        "## Request Body\n",
        "\n",
        "### Parameters\n",
        "\n",
        "| Name             | Type             | Required | Description                                                                                                                                                          |\n",
        "|------------------|------------------|----------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **model**        | string           | Required | The model to use (e.g., `claude-3-5-sonnet-20241022`).                                                                                                               |\n",
        "| **messages**     | object[]         | Required | A list of input messages. Each message must include a `role` (user or assistant) and `content`.                                                                     |\n",
        "| **max_tokens**   | integer          | Required | The maximum number of tokens to generate before stopping.                                                                                                           |\n",
        "| **system**       | string           | Optional | A system prompt providing context or instructions for the conversation.                                                                                             |\n",
        "| **stop_sequences**| string[]        | Optional | Custom sequences that will cause the model to stop generating.                                                                                                      |\n",
        "| **temperature**  | number           | Optional | Defaults to `1.0`. Controls the randomness of the output. Use values closer to `0.0` for deterministic outputs and higher values for creative tasks.                |\n",
        "| **stream**       | boolean          | Optional | Enables incremental streaming of responses using server-sent events.                                                                                                |\n",
        "| **metadata**     | object           | Optional | Metadata describing the request (e.g., tags or additional identifiers).                                                                                             |\n",
        "| **tools**        | object[]         | Optional | A list of tools the model may use. Each tool must include a `name`, `description`, and `input_schema`.                                                              |\n",
        "| **tool_choice**  | object           | Optional | Specifies how the model should use the provided tools. Options: `auto`, `any`, or specify a specific tool.                                                          |\n",
        "| **top_k**        | integer          | Optional | Limits token sampling to the top K most probable tokens.                                                                                                            |\n",
        "| **top_p**        | number           | Optional | Implements nucleus sampling. Limits token sampling to a cumulative probability mass of `top_p`.                                                                    |\n",
        "\n",
        "---\n",
        "\n",
        "## Response\n",
        "\n",
        "The API returns either a **message object** or an **error object**.\n",
        "\n",
        "### Message Object Structure\n",
        "\n",
        "| Field             | Type             | Description                                                                                                             |\n",
        "|-------------------|------------------|-------------------------------------------------------------------------------------------------------------------------|\n",
        "| **id**            | string           | A unique identifier for the message.                                                                                   |\n",
        "| **type**          | string           | Always `message`.                                                                                                      |\n",
        "| **role**          | string           | The role of the generated message. Always `assistant`.                                                                 |\n",
        "| **content**       | object[]         | An array of content blocks generated by the model. Each block has a `type` (e.g., `text`) and the corresponding `text`. |\n",
        "| **model**         | string           | The model that processed the request.                                                                                  |\n",
        "| **stop_reason**   | string or null   | The reason for stopping. Possible values: `end_turn`, `max_tokens`, `stop_sequence`, or `tool_use`.                     |\n",
        "| **stop_sequence** | string or null   | The custom stop sequence that triggered stopping, if applicable.                                                       |\n",
        "| **usage**         | object           | Token usage information. Includes `input_tokens` and `output_tokens`.                                                  |\n",
        "\n",
        "#### Token Usage\n",
        "\n",
        "| Field            | Type    | Description                              |\n",
        "|------------------|---------|------------------------------------------|\n",
        "| **input_tokens** | integer | Number of tokens in the input messages.  |\n",
        "| **output_tokens**| integer | Number of tokens in the generated output.|\n",
        "\n",
        "---\n",
        "\n",
        "### Error Object Structure\n",
        "\n",
        "| Field            | Type    | Description                     |\n",
        "|------------------|---------|---------------------------------|\n",
        "| **type**         | string  | Always `error`.                |\n",
        "| **error**        | object  | Details of the error.          |\n",
        "| **error.type**   | string  | The type of error encountered. |\n",
        "| **error.message**| string  | A detailed error message.      |\n",
        "\n",
        "---\n",
        "\n",
        "## Example cURL Request\n",
        "\n",
        "### 1. Basic Conversation\n",
        "\n",
        "```bash\n",
        "curl https://api.anthropic.com/v1/messages \\\n",
        "     --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n",
        "     --header \"anthropic-version: 2023-06-01\" \\\n",
        "     --header \"content-type: application/json\" \\\n",
        "     --data \\\n",
        "'{\n",
        "    \"model\": \"claude-3-5-sonnet-20241022\",\n",
        "    \"max_tokens\": 1024,\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Hello, world\"}\n",
        "    ]\n",
        "}'\n",
        "```\n",
        "\n",
        "**Response:**\n",
        "```json\n",
        "{\n",
        "  \"content\": [\n",
        "    {\n",
        "      \"text\": \"Hi! My name is Claude.\",\n",
        "      \"type\": \"text\"\n",
        "    }\n",
        "  ],\n",
        "  \"id\": \"msg_013Zva2CMHLNnXjNJJKqJ2EF\",\n",
        "  \"model\": \"claude-3-5-sonnet-20241022\",\n",
        "  \"role\": \"assistant\",\n",
        "  \"stop_reason\": \"end_turn\",\n",
        "  \"stop_sequence\": null,\n",
        "  \"type\": \"message\",\n",
        "  \"usage\": {\n",
        "    \"input_tokens\": 2095,\n",
        "    \"output_tokens\": 503\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### 2. Error Handling\n",
        "\n",
        "**Response Example:**\n",
        "```json\n",
        "{\n",
        "  \"type\": \"error\",\n",
        "  \"error\": {\n",
        "    \"type\": \"invalid_request_error\",\n",
        "    \"message\": \"Invalid model name\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Notes\n",
        "\n",
        "1. **Conversation Context**:\n",
        "   - Specify prior conversational turns in the `messages` parameter for multi-turn interactions.\n",
        "   - Consecutive `user` or `assistant` turns are combined automatically.\n",
        "\n",
        "2. **Input Formats**:\n",
        "   - Input messages can include both text and images (starting with Claude 3 models).\n",
        "   - Use `base64` encoding for images.\n",
        "\n",
        "3. **System Prompts**:\n",
        "   - Use the `system` parameter to provide context or instructions, such as defining the assistant's role or task.\n",
        "\n",
        "4. **Tool Integration**:\n",
        "   - Define tools the model can invoke for advanced workflows (e.g., JSON-based tools for fetching external data).\n",
        "\n",
        "5. **Token Limits**:\n",
        "   - Ensure your `max_tokens` value does not exceed the model's maximum token capacity.\n",
        "\n",
        "6. **Streaming**:\n",
        "   - Set `stream: true` to receive responses incrementally as they are generated.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Streaming Responses\n",
        "\n",
        "The **streaming** option allows you to receive messages incrementally as they are generated by the model. Streaming responses are delivered as **Server-Sent Events (SSEs)**, which allow for low-latency updates during the response generation.\n",
        "\n",
        "---\n",
        "\n",
        "## Example cURL Request with Streaming\n",
        "\n",
        "This example sends a single message and streams the response incrementally.\n",
        "\n",
        "```bash\n",
        "curl https://api.anthropic.com/v1/messages \\\n",
        "     --header \"anthropic-version: 2023-06-01\" \\\n",
        "     --header \"content-type: application/json\" \\\n",
        "     --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n",
        "     --data \\\n",
        "'{\n",
        "  \"model\": \"claude-3-5-sonnet-20241022\",\n",
        "  \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "  \"max_tokens\": 256,\n",
        "  \"stream\": true\n",
        "}'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Example Streaming Response\n",
        "\n",
        "The response is streamed as **events**. Each event provides a specific type of update, such as the start of the message, incremental text updates, or the completion of the message.\n",
        "\n",
        "```plaintext\n",
        "event: message_start\n",
        "data: {\"type\": \"message_start\", \"message\": {\"id\": \"msg_1nZdL29xx5MUA1yADyHTEsnR8uuvGzszyY\", \"type\": \"message\", \"role\": \"assistant\", \"content\": [], \"model\": \"claude-3-5-sonnet-20241022\", \"stop_reason\": null, \"stop_sequence\": null, \"usage\": {\"input_tokens\": 25, \"output_tokens\": 1}}}\n",
        "\n",
        "event: content_block_start\n",
        "data: {\"type\": \"content_block_start\", \"index\": 0, \"content_block\": {\"type\": \"text\", \"text\": \"\"}}\n",
        "\n",
        "event: ping\n",
        "data: {\"type\": \"ping\"}\n",
        "\n",
        "event: content_block_delta\n",
        "data: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"text_delta\", \"text\": \"Hello\"}}\n",
        "\n",
        "event: content_block_delta\n",
        "data: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"text_delta\", \"text\": \"!\"}}\n",
        "\n",
        "event: content_block_stop\n",
        "data: {\"type\": \"content_block_stop\", \"index\": 0}\n",
        "\n",
        "event: message_delta\n",
        "data: {\"type\": \"message_delta\", \"delta\": {\"stop_reason\": \"end_turn\", \"stop_sequence\":null}, \"usage\": {\"output_tokens\": 15}}\n",
        "\n",
        "event: message_stop\n",
        "data: {\"type\": \"message_stop\"}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Event Types in Streaming Response\n",
        "\n",
        "| Event Name               | Description                                                                                       |\n",
        "|--------------------------|---------------------------------------------------------------------------------------------------|\n",
        "| **message_start**        | Indicates the beginning of the message response. Contains initial metadata like the message ID.   |\n",
        "| **content_block_start**  | Marks the start of a content block (e.g., text or image).                                         |\n",
        "| **ping**                 | A periodic ping sent to maintain the connection.                                                 |\n",
        "| **content_block_delta**  | Contains incremental updates (deltas) to the content block.                                       |\n",
        "| **content_block_stop**   | Indicates the end of a content block.                                                            |\n",
        "| **message_delta**        | Provides updates to the message's metadata (e.g., stop reason, token usage).                     |\n",
        "| **message_stop**         | Marks the end of the streaming response.                                                         |\n",
        "\n",
        "---\n",
        "\n",
        "## Notes\n",
        "\n",
        "1. **Streaming Activation**:\n",
        "   - Set `\"stream\": true` in the request body to enable streaming responses.\n",
        "\n",
        "2. **Latency**:\n",
        "   - Streaming is ideal for low-latency applications, as content is delivered as soon as it is generated.\n",
        "\n",
        "3. **Connection Handling**:\n",
        "   - Streaming responses are sent as a continuous stream. Ensure your client can process **Server-Sent Events (SSEs)**.\n",
        "\n",
        "4. **Event Order**:\n",
        "   - Events are delivered in the following order:\n",
        "     - `message_start` → `content_block_start` → `content_block_delta` → `content_block_stop` → `message_delta` → `message_stop`.\n",
        "\n",
        "5. **Usage Statistics**:\n",
        "   - Token usage (`input_tokens` and `output_tokens`) is updated dynamically in the `message_delta` and `message_stop` events.\n",
        "\n",
        "6. **Partial Responses**:\n",
        "   - You can use the `content_block_delta` events to display partial responses to users in real-time.\n"
      ],
      "metadata": {
        "id": "YRB-aWTsQxgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VOYAGE Embedding\n",
        "\n",
        "The **Voyage Embeddings API** generates vector embeddings for input text using specialized models. These embeddings can be used for tasks such as similarity comparison, clustering, and retrieval in domain-specific applications.\n",
        "\n",
        "---\n",
        "\n",
        "## Endpoint\n",
        "```\n",
        "POST https://api.voyageai.com/v1/embeddings\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Authorization\n",
        "\n",
        "**HTTP Authorization Scheme**: `bearer`  \n",
        "**Header**:  \n",
        "```\n",
        "Authorization: Bearer <VOYAGE_API_KEY>\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Request Body Parameters\n",
        "\n",
        "The request body must be sent in **JSON format** with the following fields:\n",
        "\n",
        "### Required Parameters\n",
        "\n",
        "| Parameter       | Type                       | Description                                                                                                        |\n",
        "|-----------------|----------------------------|--------------------------------------------------------------------------------------------------------------------|\n",
        "| **input**       | string or array of strings | Text input for embedding. Supports a single string or a list of strings (max 128). See constraints in the notes.   |\n",
        "| **model**       | string                     | The name of the model to use. Recommended options: `voyage-3`, `voyage-3-lite`, `voyage-finance-2`, etc.           |\n",
        "\n",
        "### Optional Parameters\n",
        "\n",
        "| Parameter       | Type     | Default | Description                                                                                                        |\n",
        "|-----------------|----------|---------|--------------------------------------------------------------------------------------------------------------------|\n",
        "| **input_type**  | string   | null    | Specifies the type of input text. Options: `query`, `document`.                                                    |\n",
        "| **truncation**  | boolean  | true    | Whether to truncate input texts to fit the model's context length. If `false`, overly long texts will raise an error. |\n",
        "| **encoding_format** | string | null   | Output format for embeddings. Options: `null` (list of floats), `base64` (compressed base64 encodings).             |\n",
        "\n",
        "---\n",
        "\n",
        "## Constraints\n",
        "\n",
        "1. **List Size**:\n",
        "   - Maximum of **128 texts** per request.\n",
        "2. **Token Limits**:\n",
        "   - **voyage-3-lite**: 1M tokens.\n",
        "   - **voyage-3** and **voyage-2**: 320K tokens.\n",
        "   - Domain-specific models (e.g., `voyage-law-2`, `voyage-finance-2`): 120K tokens.\n",
        "3. **Text Length**:\n",
        "   - Input texts exceeding the model's context length must either be truncated (if `truncation: true`) or will raise an error.\n",
        "\n",
        "---\n",
        "\n",
        "## Example Request\n",
        "\n",
        "### Basic Request\n",
        "\n",
        "```bash\n",
        "curl --request POST \\\n",
        "     --url https://api.voyageai.com/v1/embeddings \\\n",
        "     --header \"Authorization: Bearer $VOYAGE_API_KEY\" \\\n",
        "     --header \"content-type: application/json\" \\\n",
        "     --data '{\n",
        "       \"input\": [\n",
        "         \"Sample text 1\",\n",
        "         \"Sample text 2\"\n",
        "       ],\n",
        "       \"model\": \"voyage-large-2\"\n",
        "     }'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Example Response\n",
        "\n",
        "### Success Response (200)\n",
        "\n",
        "| Field              | Type                | Description                                                                 |\n",
        "|--------------------|---------------------|-----------------------------------------------------------------------------|\n",
        "| **object**         | string              | Always `\"list\"`.                                                            |\n",
        "| **data**           | array of objects    | A list of embeddings generated for each input text.                         |\n",
        "| **model**          | string              | The model used for generating embeddings.                                   |\n",
        "| **usage**          | object              | Contains token usage statistics (`total_tokens`).                           |\n",
        "\n",
        "#### Response Example\n",
        "```json\n",
        "{\n",
        "  \"object\": \"list\",\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"object\": \"embedding\",\n",
        "      \"embedding\": [\n",
        "        0.0038915484,\n",
        "        0.010964915,\n",
        "        -0.035594109,\n",
        "        \"...\",\n",
        "        0.011034692\n",
        "      ],\n",
        "      \"index\": 0\n",
        "    },\n",
        "    {\n",
        "      \"object\": \"embedding\",\n",
        "      \"embedding\": [\n",
        "        -0.01539533,\n",
        "        -0.0011246679,\n",
        "        0.021264801,\n",
        "        \"...\",\n",
        "        -0.046319865\n",
        "      ],\n",
        "      \"index\": 1\n",
        "    }\n",
        "  ],\n",
        "  \"model\": \"voyage-large-2\",\n",
        "  \"usage\": {\n",
        "    \"total_tokens\": 10\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "### Embedding Object Structure\n",
        "\n",
        "| Field          | Type              | Description                                                                 |\n",
        "|----------------|-------------------|-----------------------------------------------------------------------------|\n",
        "| **object**     | string            | Always `\"embedding\"`.                                                      |\n",
        "| **embedding**  | array of numbers  | The generated embedding vector as a list of floating-point numbers.        |\n",
        "| **index**      | integer           | Index of the embedding in the input list.                                  |\n",
        "\n",
        "---\n",
        "\n",
        "### Error Responses\n",
        "\n",
        "#### Client Error (4XX)\n",
        "Indicates an issue with the request format or constraints (e.g., too many tokens or invalid input).\n",
        "\n",
        "| Field          | Type                | Description                                                                 |\n",
        "|----------------|---------------------|-----------------------------------------------------------------------------|\n",
        "| **detail**     | array of objects    | List of errors explaining the issue.                                       |\n",
        "| **loc**        | array of strings    | Location of the error (e.g., `\"body\", \"input\"`).                           |\n",
        "| **msg**        | string              | Error message.                                                             |\n",
        "| **type**       | string              | Type of error (e.g., `\"validation_error\"`).                                 |\n",
        "\n",
        "#### Server Error (5XX)\n",
        "Indicates server-side issues such as high traffic or unexpected failures.\n",
        "\n",
        "---\n",
        "\n",
        "## Notes\n",
        "\n",
        "1. **Model Options**:\n",
        "   - General-purpose: `voyage-3`, `voyage-3-lite`.\n",
        "   - Domain-specific: `voyage-finance-2`, `voyage-law-2`, `voyage-multilingual-2`, `voyage-code-2`.\n",
        "2. **Truncation**:\n",
        "   - Ensure `truncation: true` for long texts, or validate text lengths beforehand.\n",
        "3. **Embedding Format**:\n",
        "   - Default: Floating-point vectors.\n",
        "   - Use `encoding_format: \"base64\"` for compressed embeddings.\n",
        "4. **Token Usage**:\n",
        "   - The `usage.total_tokens` field provides insights into billing and token efficiency.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "m71nVzajRCZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VOYAGE Multimodal Embedding\n",
        "\n",
        "The **Voyage Multimodal Embeddings API** generates vector representations for inputs containing text, images, or a combination of both. These embeddings can be used for retrieval, search, or clustering in applications requiring multimodal data handling.\n",
        "\n",
        "---\n",
        "\n",
        "## Endpoint\n",
        "```\n",
        "POST https://api.voyageai.com/v1/multimodalembeddings\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Authorization\n",
        "\n",
        "**HTTP Authorization Scheme**: `bearer`  \n",
        "**Header**:  \n",
        "```\n",
        "Authorization: Bearer <VOYAGE_API_KEY>\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Request Body Parameters\n",
        "\n",
        "The request body must be sent in **JSON format** with the following fields:\n",
        "\n",
        "### Required Parameters\n",
        "\n",
        "| Parameter           | Type                | Description                                                                                                                                 |\n",
        "|---------------------|---------------------|---------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **inputs**          | array of objects    | A list of multimodal inputs, each containing a sequence of text and/or images. Each item must have a `content` key, explained below.        |\n",
        "| **model**           | string              | The model name. Currently, only `\"voyage-multimodal-3\"` is supported.                                                                      |\n",
        "\n",
        "### Optional Parameters\n",
        "\n",
        "| Parameter           | Type                | Default        | Description                                                                                                                                 |\n",
        "|---------------------|---------------------|----------------|---------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **input_type**      | string              | null           | Input type for retrieval tasks. Options: `query`, `document`. See the **Notes** section for more details.                                  |\n",
        "| **truncation**      | boolean             | true           | Whether to truncate inputs that exceed the context length. If `false`, an error is raised for over-length inputs.                          |\n",
        "| **output_encoding** | string              | null           | Format for output embeddings. Options: `null` (list of floats), `base64` (Base64-encoded NumPy array).                                     |\n",
        "\n",
        "---\n",
        "\n",
        "## Input Format\n",
        "\n",
        "The **inputs** parameter accepts a list of dictionaries. Each dictionary must have a `content` key with a list of dictionaries, representing the sequence of text and images.\n",
        "\n",
        "### Content Dictionary Keys\n",
        "\n",
        "| Key            | Type              | Description                                                                                 |\n",
        "|----------------|-------------------|---------------------------------------------------------------------------------------------|\n",
        "| **type**       | string            | The type of content. Allowed values: `text`, `image_url`, `image_base64`.                   |\n",
        "| **text**       | string (optional) | The text string (if `type` is `text`).                                                     |\n",
        "| **image_url**  | string (optional) | URL of the image (if `type` is `image_url`). Supported formats: PNG, JPEG, WEBP, GIF.       |\n",
        "| **image_base64**| string (optional) | Base64-encoded image in `data:[<mediatype>];base64,<data>` format (if `type` is `image_base64`). |\n",
        "\n",
        "### Notes on Image Usage\n",
        "- Use **either** `image_url` or `image_base64` for images within a single request, not both.\n",
        "- Image constraints:\n",
        "  - Maximum 16 million pixels per image.\n",
        "  - Maximum image size: 20 MB.\n",
        "\n",
        "---\n",
        "\n",
        "## Constraints\n",
        "\n",
        "1. **List Size**:  \n",
        "   - Maximum 1000 inputs per request.\n",
        "\n",
        "2. **Token and Pixel Limits**:  \n",
        "   - Each input:\n",
        "     - Must not exceed **32,000 tokens**.\n",
        "   - Total inputs:\n",
        "     - Must not exceed **320,000 tokens**.  \n",
        "     - Images: Every 560 pixels count as one token.\n",
        "\n",
        "3. **Truncation**:\n",
        "   - If `truncation: true`, over-length inputs are truncated to fit the context length. Entire images are discarded if truncation occurs mid-image.\n",
        "\n",
        "---\n",
        "\n",
        "## Example Request\n",
        "\n",
        "### Example with Text and Image URL\n",
        "\n",
        "```bash\n",
        "curl -X POST https://api.voyageai.com/v1/multimodalembeddings \\\n",
        "  -H \"Authorization: Bearer $VOYAGE_API_KEY\" \\\n",
        "  -H \"content-type: application/json\" \\\n",
        "  -d '{\n",
        "    \"inputs\": [\n",
        "      {\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": \"This is a banana.\"\n",
        "          },\n",
        "          {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": \"https://raw.githubusercontent.com/voyage-ai/voyage-multimodal-3/refs/heads/main/images/banana.jpg\"\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    \"model\": \"voyage-multimodal-3\"\n",
        "  }'\n",
        "```\n",
        "\n",
        "### Example with Base64 Image\n",
        "\n",
        "```bash\n",
        "curl -X POST https://api.voyageai.com/v1/multimodalembeddings \\\n",
        "  -H \"Authorization: Bearer $VOYAGE_API_KEY\" \\\n",
        "  -H \"content-type: application/json\" \\\n",
        "  -d '{\n",
        "    \"inputs\": [\n",
        "      {\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": \"This is a banana.\"\n",
        "          },\n",
        "          {\n",
        "            \"type\": \"image_base64\",\n",
        "            \"image_base64\": \"data:image/jpeg;base64,/9j/4AAQSkZJRg...\"\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    \"model\": \"voyage-multimodal-3\"\n",
        "  }'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Example Response\n",
        "\n",
        "### Success Response (200)\n",
        "\n",
        "| Field              | Type                | Description                                                                 |\n",
        "|--------------------|---------------------|-----------------------------------------------------------------------------|\n",
        "| **object**         | string              | Always `\"list\"`.                                                            |\n",
        "| **data**           | array of objects    | List of embedding objects for each input.                                   |\n",
        "| **model**          | string              | The model used for generating embeddings.                                   |\n",
        "| **usage**          | object              | Token and pixel usage details: `text_tokens`, `image_pixels`, `total_tokens`. |\n",
        "\n",
        "#### Response Example\n",
        "```json\n",
        "{\n",
        "  \"object\": \"list\",\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"object\": \"embedding\",\n",
        "      \"embedding\": [\n",
        "        0.027587891,\n",
        "        -0.021240234,\n",
        "        0.018310547,\n",
        "        \"...\",\n",
        "        -0.021240234\n",
        "      ],\n",
        "      \"index\": 0\n",
        "    }\n",
        "  ],\n",
        "  \"model\": \"voyage-multimodal-3\",\n",
        "  \"usage\": {\n",
        "    \"text_tokens\": 5,\n",
        "    \"image_pixels\": 2000000,\n",
        "    \"total_tokens\": 3576\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Notes\n",
        "\n",
        "1. **Model Options**:\n",
        "   - Currently, only `voyage-multimodal-3` is supported.\n",
        "\n",
        "2. **Input Type (`input_type`)**:\n",
        "   - **`query`**: Use for retrieval/search queries.\n",
        "   - **`document`**: Use for relevant supporting information.\n",
        "   - Prompts are prepended to the input for these modes to improve performance.\n",
        "\n",
        "3. **Truncation**:\n",
        "   - Ensure `truncation: true` for seamless processing of over-length inputs.\n",
        "   - Images truncated mid-way are entirely discarded.\n",
        "\n",
        "4. **Token Calculation**:\n",
        "   - Every 560 image pixels count as **1 token**.\n",
        "\n",
        "5. **Embedding Format**:\n",
        "   - Default: List of floating-point numbers.\n",
        "   - Use `output_encoding: \"base64\"` for compressed NumPy arrays.\n",
        "\n",
        "6. **Error Handling**:\n",
        "   - **4XX Errors**: Issues with request formatting or exceeding constraints.\n",
        "   - **5XX Errors**: Server-side issues such as high traffic or unexpected failures.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dhG2_C2cRLFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VOYAGE Rerank\n",
        "\n",
        "The **Voyage Reranker API** evaluates and ranks documents based on their relevance to a given query. It supports various models optimized for different use cases and performance requirements.\n",
        "\n",
        "---\n",
        "\n",
        "## Endpoint\n",
        "```\n",
        "POST https://api.voyageai.com/v1/rerank\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Authorization\n",
        "\n",
        "**HTTP Authorization Scheme**: `bearer`  \n",
        "**Header**:  \n",
        "```\n",
        "Authorization: Bearer <VOYAGE_API_KEY>\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Request Body Parameters\n",
        "\n",
        "The request body must be sent in **JSON format** with the following fields:\n",
        "\n",
        "### Required Parameters\n",
        "\n",
        "| Parameter       | Type                | Description                                                                                                        |\n",
        "|-----------------|---------------------|--------------------------------------------------------------------------------------------------------------------|\n",
        "| **query**       | string              | The query string to evaluate relevance. Token limits depend on the model (see **Constraints** below).              |\n",
        "| **documents**   | array of strings    | A list of documents to be reranked. Token and document count constraints apply (see **Constraints** below).         |\n",
        "| **model**       | string              | The model name to use for reranking. Recommended options: `rerank-2`, `rerank-2-lite`.                             |\n",
        "\n",
        "### Optional Parameters\n",
        "\n",
        "| Parameter           | Type      | Default  | Description                                                                                                        |\n",
        "|---------------------|-----------|----------|--------------------------------------------------------------------------------------------------------------------|\n",
        "| **top_k**           | integer   | null     | The number of most relevant documents to return. If not specified, all reranked documents are returned.            |\n",
        "| **return_documents**| boolean   | false    | Whether to return the document content along with relevance scores.                                                |\n",
        "| **truncation**      | boolean   | true     | Whether to truncate the query and documents to fit within the context length. If `false`, exceeding limits raises an error. |\n",
        "\n",
        "---\n",
        "\n",
        "## Constraints\n",
        "\n",
        "1. **Query Length**:\n",
        "   - Maximum tokens:\n",
        "     - `rerank-2`: **4000 tokens**.\n",
        "     - `rerank-2-lite`, `rerank-1`: **2000 tokens**.\n",
        "     - `rerank-lite-1`: **1000 tokens**.\n",
        "\n",
        "2. **Document Count**:\n",
        "   - Maximum **1000 documents**.\n",
        "\n",
        "3. **Token Limits**:\n",
        "   - The total tokens for a query and any single document cannot exceed:\n",
        "     - `rerank-2`: **16,000 tokens**.\n",
        "     - `rerank-2-lite`, `rerank-1`: **8000 tokens**.\n",
        "     - `rerank-lite-1`: **4000 tokens**.\n",
        "   - The total tokens across all query and documents cannot exceed **300,000 tokens**. Calculated as:\n",
        "     ```\n",
        "     (Query tokens × Number of documents) + Sum of document tokens\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "## Example Request\n",
        "\n",
        "### Basic Rerank Request\n",
        "\n",
        "```bash\n",
        "curl --request POST \\\n",
        "     --url https://api.voyageai.com/v1/rerank \\\n",
        "     --header \"Authorization: Bearer $VOYAGE_API_KEY\" \\\n",
        "     --header \"content-type: application/json\" \\\n",
        "     --data '{\n",
        "       \"query\": \"Sample query\",\n",
        "       \"documents\": [\n",
        "         \"Sample document 1\",\n",
        "         \"Sample document 2\"\n",
        "       ],\n",
        "       \"model\": \"rerank-lite-1\"\n",
        "     }'\n",
        "```\n",
        "\n",
        "### Request with Top-K Results and Document Return\n",
        "\n",
        "```bash\n",
        "curl --request POST \\\n",
        "     --url https://api.voyageai.com/v1/rerank \\\n",
        "     --header \"Authorization: Bearer $VOYAGE_API_KEY\" \\\n",
        "     --header \"content-type: application/json\" \\\n",
        "     --data '{\n",
        "       \"query\": \"Find the most relevant document\",\n",
        "       \"documents\": [\n",
        "         \"This is document A.\",\n",
        "         \"This is document B.\"\n",
        "       ],\n",
        "       \"model\": \"rerank-2\",\n",
        "       \"top_k\": 1,\n",
        "       \"return_documents\": true\n",
        "     }'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Example Response\n",
        "\n",
        "### Success Response (200)\n",
        "\n",
        "| Field              | Type                | Description                                                                 |\n",
        "|--------------------|---------------------|-----------------------------------------------------------------------------|\n",
        "| **object**         | string              | Always `\"list\"`.                                                            |\n",
        "| **data**           | array of objects    | Array of reranked results, sorted by descending relevance score.            |\n",
        "| **model**          | string              | The model used for reranking.                                               |\n",
        "| **usage**          | object              | Token usage statistics, including `total_tokens`.                           |\n",
        "\n",
        "#### Response Example (Without Documents)\n",
        "```json\n",
        "{\n",
        "  \"object\": \"list\",\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"relevance_score\": 0.4375,\n",
        "      \"index\": 0\n",
        "    },\n",
        "    {\n",
        "      \"relevance_score\": 0.421875,\n",
        "      \"index\": 1\n",
        "    }\n",
        "  ],\n",
        "  \"model\": \"rerank-lite-1\",\n",
        "  \"usage\": {\n",
        "    \"total_tokens\": 26\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### Response Example (With Documents)\n",
        "```json\n",
        "{\n",
        "  \"object\": \"list\",\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"relevance_score\": 0.675,\n",
        "      \"index\": 0,\n",
        "      \"document\": \"This is document A.\"\n",
        "    }\n",
        "  ],\n",
        "  \"model\": \"rerank-2\",\n",
        "  \"usage\": {\n",
        "    \"total_tokens\": 50\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Notes\n",
        "\n",
        "1. **Model Options**:\n",
        "   - **`rerank-2`**: Best for high token limits (up to 4000 tokens per query, 16,000 combined).\n",
        "   - **`rerank-2-lite`**: Optimized for lower token limits.\n",
        "   - **`rerank-lite-1`**: Lightweight model for minimal token requirements.\n",
        "\n",
        "2. **Token Truncation**:\n",
        "   - Enable `truncation: true` to automatically handle over-length inputs.\n",
        "   - If `truncation: false`, errors are raised for queries/documents exceeding token limits.\n",
        "\n",
        "3. **Returning Documents**:\n",
        "   - Set `return_documents: true` to include document text in the response alongside relevance scores.\n",
        "\n",
        "4. **Usage Statistics**:\n",
        "   - Token usage in the `usage` object helps monitor costs and optimize queries.\n",
        "\n",
        "5. **Relevance Scoring**:\n",
        "   - Results are sorted by `relevance_score` in descending order.\n",
        "\n",
        "---\n",
        "\n",
        "## Error Responses\n",
        "\n",
        "### Client Error (4XX)\n",
        "Indicates issues with the request, such as exceeding token or document limits.\n",
        "\n",
        "| Field              | Type          | Description                                                                 |\n",
        "|--------------------|---------------|-----------------------------------------------------------------------------|\n",
        "| **type**           | string        | The type of error (e.g., `\"validation_error\"`).                             |\n",
        "| **message**        | string        | A detailed error message.                                                  |\n",
        "\n",
        "#### Example\n",
        "```json\n",
        "{\n",
        "  \"type\": \"validation_error\",\n",
        "  \"message\": \"Query exceeds maximum token limit for the selected model.\"\n",
        "}\n",
        "```\n",
        "\n",
        "### Server Error (5XX)\n",
        "Indicates server-side issues such as high traffic or unexpected failures.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "r69vGmiNROow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPENAI Chat Completion\n",
        "\n",
        "The **Create Chat Completion API** generates responses for a given conversation using a specified model.\n",
        "\n",
        "---\n",
        "\n",
        "## Endpoint\n",
        "```\n",
        "POST https://api.openai.com/v1/chat/completions\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Request Body\n",
        "\n",
        "### Parameters\n",
        "\n",
        "| Name                     | Type            | Required | Description                                                                                              |\n",
        "|--------------------------|-----------------|----------|----------------------------------------------------------------------------------------------------------|\n",
        "| **messages**             | array           | Required | A list of messages in the conversation so far. Supported types include `text`, `images`, and `audio`. |\n",
        "| **model**                | string          | Required | ID of the model to use. See the model endpoint compatibility table for available models.                |\n",
        "| **store**                | boolean/null    | Optional | Defaults to `false`. Whether to store the output of this request for use in model distillation or evals. |\n",
        "| **metadata**             | object/null     | Optional | Developer-defined tags/values for filtering completions in the dashboard.                               |\n",
        "| **frequency_penalty**    | number/null     | Optional | Defaults to `0`. Penalizes new tokens based on their frequency. Range: `-2.0` to `2.0`.                 |\n",
        "| **logit_bias**           | map             | Optional | Modify the likelihood of specific tokens. Maps token IDs to bias values from `-100` to `100`.           |\n",
        "| **logprobs**             | boolean/null    | Optional | Defaults to `false`. Returns log probabilities of output tokens if set to `true`.                       |\n",
        "| **top_logprobs**         | integer/null    | Optional | Specifies the number of top tokens with log probabilities to return. Requires `logprobs: true`.         |\n",
        "| **max_completion_tokens**| integer/null    | Optional | Upper bound for tokens generated in the response. Replaces the deprecated `max_tokens`.                 |\n",
        "| **n**                    | integer/null    | Optional | Defaults to `1`. Specifies the number of completion choices to generate for each input message.         |\n",
        "| **modalities**           | array/null      | Optional | Output types to generate (e.g., `[\"text\"]`, or `[\"text\", \"audio\"]` for multi-modal responses).           |\n",
        "| **prediction**           | object          | Optional | Configuration for predicted outputs to improve response times for known content.                        |\n",
        "| **audio**                | object/null     | Optional | Parameters for audio output. Required for `modalities: [\"audio\"]`.                                      |\n",
        "| **presence_penalty**     | number/null     | Optional | Defaults to `0`. Penalizes tokens based on their presence. Range: `-2.0` to `2.0`.                      |\n",
        "| **response_format**      | object          | Optional | Specifies the output format (e.g., `json_schema`, `json_object`).                                        |\n",
        "| **seed**                 | integer/null    | Optional | Ensures determinism in sampling. Responses may vary slightly but aim to be consistent.                  |\n",
        "| **service_tier**         | string/null     | Optional | Defaults to `auto`. Specifies the service tier for processing requests (e.g., `scale` or `default`).    |\n",
        "| **stop**                 | string/array/null | Optional | Defaults to `null`. Up to 4 sequences where the API will stop generating further tokens.                 |\n",
        "| **stream**               | boolean/null    | Optional | Defaults to `false`. Enables token streaming as server-sent events (`data: [DONE]`).                     |\n",
        "| **temperature**          | number/null     | Optional | Defaults to `1`. Sampling temperature (0-2). Higher values increase randomness.                         |\n",
        "| **top_p**                | number/null     | Optional | Defaults to `1`. Uses nucleus sampling. Tokens within the `top_p` probability mass are considered.      |\n",
        "| **tools**                | array           | Optional | List of tools (e.g., functions) the model can call. Max of 128 tools.                                   |\n",
        "| **tool_choice**          | string/object   | Optional | Controls how tools are used (`none`, `auto`, or specify a tool).                                         |\n",
        "| **parallel_tool_calls**  | boolean         | Optional | Defaults to `true`. Enables parallel function calling during tool use.                                  |\n",
        "| **user**                 | string          | Optional | Unique identifier for monitoring end-user behavior.                                                     |\n",
        "\n",
        "---\n",
        "\n",
        "## Returns\n",
        "\n",
        "- **Chat Completion Object**: Contains the model's response and metadata.\n",
        "- **Streamed Chat Completion**: A sequence of token chunks when `stream: true`.\n",
        "\n",
        "---\n",
        "\n",
        "## Example cURL Requests\n",
        "\n",
        "### 1. Basic Chat Completion\n",
        "```bash\n",
        "curl https://api.openai.com/v1/chat/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -d '{\n",
        "    \"model\": \"gpt-4o\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant.\"\n",
        "      },\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Hello!\"\n",
        "      }\n",
        "    ]\n",
        "  }'\n",
        "```\n",
        "\n",
        "**Response:**\n",
        "```json\n",
        "{\n",
        "  \"id\": \"chatcmpl-123\",\n",
        "  \"object\": \"chat.completion\",\n",
        "  \"created\": 1677652288,\n",
        "  \"model\": \"gpt-4o-mini\",\n",
        "  \"system_fingerprint\": \"fp_44709d6fcb\",\n",
        "  \"choices\": [{\n",
        "    \"index\": 0,\n",
        "    \"message\": {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Hello there, how may I assist you today?\"\n",
        "    },\n",
        "    \"finish_reason\": \"stop\"\n",
        "  }],\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 9,\n",
        "    \"completion_tokens\": 12,\n",
        "    \"total_tokens\": 21\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Multi-Modal Chat Completion (Text and Image)\n",
        "```bash\n",
        "curl https://api.openai.com/v1/chat/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -d '{\n",
        "    \"model\": \"gpt-4o\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": \"What's in this image?\"\n",
        "          },\n",
        "          {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\n",
        "              \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
        "            }\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    \"max_completion_tokens\": 300\n",
        "  }'\n",
        "```\n",
        "\n",
        "**Response:**\n",
        "```json\n",
        "{\n",
        "  \"id\": \"chatcmpl-123\",\n",
        "  \"object\": \"chat.completion\",\n",
        "  \"created\": 1677652288,\n",
        "  \"model\": \"gpt-4o-mini\",\n",
        "  \"choices\": [{\n",
        "    \"index\": 0,\n",
        "    \"message\": {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"This image shows a wooden boardwalk extending through a lush green marshland.\"\n",
        "    },\n",
        "    \"finish_reason\": \"stop\"\n",
        "  }]\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Streamed Chat Completion\n",
        "```bash\n",
        "curl https://api.openai.com/v1/chat/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -d '{\n",
        "    \"model\": \"gpt-4o\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant.\"\n",
        "      },\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Hello!\"\n",
        "      }\n",
        "    ],\n",
        "    \"stream\": true\n",
        "  }'\n",
        "```\n",
        "\n",
        "**Response:** The response will be a sequence of chunks:\n",
        "```json\n",
        "{\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-4o-mini\", \"choices\":[{\"delta\":{\"content\":\"Hello\"}}]}\n",
        "{\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-4o-mini\", \"choices\":[{\"delta\":{\"content\":\" there,\"}}]}\n",
        "...\n",
        "{\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-4o-mini\", \"choices\":[{\"delta\":{\"content\":\" how may I assist you today?\"}}]}\n",
        "{\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1694268190,\"model\":\"gpt-4o-mini\", \"choices\":[{\"finish_reason\":\"stop\"}]}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Example 4: Using Tools in Chat Completion\n",
        "\n",
        "#### Request:\n",
        "```bash\n",
        "curl https://api.openai.com/v1/chat/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -d '{\n",
        "    \"model\": \"gpt-4o\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What'\\''s the weather like in Boston today?\"\n",
        "      }\n",
        "    ],\n",
        "    \"tools\": [\n",
        "      {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "          \"name\": \"get_current_weather\",\n",
        "          \"description\": \"Get the current weather in a given location\",\n",
        "          \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "              \"location\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "              },\n",
        "              \"unit\": {\n",
        "                \"type\": \"string\",\n",
        "                \"enum\": [\"celsius\", \"fahrenheit\"]\n",
        "              }\n",
        "            },\n",
        "            \"required\": [\"location\"]\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    ],\n",
        "    \"tool_choice\": \"auto\"\n",
        "  }'\n",
        "```\n",
        "\n",
        "#### Response:\n",
        "```json\n",
        "{\n",
        "  \"id\": \"chatcmpl-abc123\",\n",
        "  \"object\": \"chat.completion\",\n",
        "  \"created\": 1699896916,\n",
        "  \"model\": \"gpt-4o-mini\",\n",
        "  \"choices\": [\n",
        "    {\n",
        "      \"index\": 0,\n",
        "      \"message\": {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": null,\n",
        "        \"tool_calls\": [\n",
        "          {\n",
        "            \"id\": \"call_abc123\",\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "              \"name\": \"get_current_weather\",\n",
        "              \"arguments\": \"{\\n\\\"location\\\": \\\"Boston, MA\\\"\\n}\"\n",
        "            }\n",
        "          }\n",
        "        ]\n",
        "      },\n",
        "      \"logprobs\": null,\n",
        "      \"finish_reason\": \"tool_calls\"\n",
        "    }\n",
        "  ],\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 82,\n",
        "    \"completion_tokens\": 17,\n",
        "    \"total_tokens\": 99,\n",
        "    \"completion_tokens_details\": {\n",
        "      \"reasoning_tokens\": 0,\n",
        "      \"accepted_prediction_tokens\": 0,\n",
        "      \"rejected_prediction_tokens\": 0\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Example 5: Including Log Probabilities\n",
        "\n",
        "#### Request:\n",
        "```bash\n",
        "curl https://api.openai.com/v1/chat/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -d '{\n",
        "    \"model\": \"gpt-4o\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Hello!\"\n",
        "      }\n",
        "    ],\n",
        "    \"logprobs\": true,\n",
        "    \"top_logprobs\": 2\n",
        "  }'\n",
        "```\n",
        "\n",
        "#### Response:\n",
        "```json\n",
        "{\n",
        "  \"id\": \"chatcmpl-123\",\n",
        "  \"object\": \"chat.completion\",\n",
        "  \"created\": 1702685778,\n",
        "  \"model\": \"gpt-4o-mini\",\n",
        "  \"choices\": [\n",
        "    {\n",
        "      \"index\": 0,\n",
        "      \"message\": {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Hello! How can I assist you today?\"\n",
        "      },\n",
        "      \"logprobs\": {\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"token\": \"Hello\",\n",
        "            \"logprob\": -0.31725305,\n",
        "            \"bytes\": [72, 101, 108, 108, 111],\n",
        "            \"top_logprobs\": [\n",
        "              {\n",
        "                \"token\": \"Hello\",\n",
        "                \"logprob\": -0.31725305,\n",
        "                \"bytes\": [72, 101, 108, 108, 111]\n",
        "              },\n",
        "              {\n",
        "                \"token\": \"Hi\",\n",
        "                \"logprob\": -1.3190403,\n",
        "                \"bytes\": [72, 105]\n",
        "              }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            \"token\": \"!\",\n",
        "            \"logprob\": -0.02380986,\n",
        "            \"bytes\": [33],\n",
        "            \"top_logprobs\": [\n",
        "              {\n",
        "                \"token\": \"!\",\n",
        "                \"logprob\": -0.02380986,\n",
        "                \"bytes\": [33]\n",
        "              },\n",
        "              {\n",
        "                \"token\": \" there\",\n",
        "                \"logprob\": -3.787621,\n",
        "                \"bytes\": [32, 116, 104, 101, 114, 101]\n",
        "              }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            \"token\": \" How\",\n",
        "            \"logprob\": -0.000054669687,\n",
        "            \"bytes\": [32, 72, 111, 119],\n",
        "            \"top_logprobs\": [\n",
        "              {\n",
        "                \"token\": \" How\",\n",
        "                \"logprob\": -0.000054669687,\n",
        "                \"bytes\": [32, 72, 111, 119]\n",
        "              },\n",
        "              {\n",
        "                \"token\": \"<|end|>\",\n",
        "                \"logprob\": -10.953937,\n",
        "                \"bytes\": null\n",
        "              }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            \"token\": \" can\",\n",
        "            \"logprob\": -0.015801601,\n",
        "            \"bytes\": [32, 99, 97, 110],\n",
        "            \"top_logprobs\": [\n",
        "              {\n",
        "                \"token\": \" can\",\n",
        "                \"logprob\": -0.015801601,\n",
        "                \"bytes\": [32, 99, 97, 110]\n",
        "              },\n",
        "              {\n",
        "                \"token\": \" may\",\n",
        "                \"logprob\": -4.161023,\n",
        "                \"bytes\": [32, 109, 97, 121]\n",
        "              }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            \"token\": \" I\",\n",
        "            \"logprob\": -3.7697225e-6,\n",
        "            \"bytes\": [32, 73],\n",
        "            \"top_logprobs\": [\n",
        "              {\n",
        "                \"token\": \" I\",\n",
        "                \"logprob\": -3.7697225e-6,\n",
        "                \"bytes\": [32, 73]\n",
        "              },\n",
        "              {\n",
        "                \"token\": \" assist\",\n",
        "                \"logprob\": -13.596657,\n",
        "                \"bytes\": [32, 97, 115, 115, 105, 115, 116]\n",
        "              }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            \"token\": \" assist\",\n",
        "            \"logprob\": -0.04571125,\n",
        "            \"bytes\": [32, 97, 115, 115, 105, 115, 116],\n",
        "            \"top_logprobs\": [\n",
        "              {\n",
        "                \"token\": \" assist\",\n",
        "                \"logprob\": -0.04571125,\n",
        "                \"bytes\": [32, 97, 115, 115, 105, 115, 116]\n",
        "              },\n",
        "              {\n",
        "                \"token\": \" help\",\n",
        "                \"logprob\": -3.1089056,\n",
        "                \"bytes\": [32, 104, 101, 108, 112]\n",
        "              }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            \"token\": \" you\",\n",
        "            \"logprob\": -5.4385737e-6,\n",
        "            \"bytes\": [32, 121, 111, 117],\n",
        "            \"top_logprobs\": [\n",
        "              {\n",
        "                \"token\": \" you\",\n",
        "                \"logprob\": -5.4385737e-6,\n",
        "                \"bytes\": [32, 121, 111, 117]\n",
        "              },\n",
        "              {\n",
        "                \"token\": \" today\",\n",
        "                \"logprob\": -12.807695,\n",
        "                \"bytes\": [32, 116, 111, 100, 97, 121]\n",
        "              }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            \"token\": \" today\",\n",
        "            \"logprob\": -0.0040071653,\n",
        "            \"bytes\": [32, 116, 111, 100, 97, 121],\n",
        "            \"top_logprobs\": [\n",
        "              {\n",
        "                \"token\": \" today\",\n",
        "                \"logprob\": -0.0040071653,\n",
        "                \"bytes\": [32, 116, 111, 100, 97, 121]\n",
        "              },\n",
        "              {\n",
        "                \"token\": \"?\",\n",
        "                \"logprob\": -5.5247097,\n",
        "                \"bytes\": [63]\n",
        "              }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            \"token\": \"?\",\n",
        "            \"logprob\": -0.0008108172,\n",
        "            \"bytes\": [63],\n",
        "            \"top_logprobs\": [\n",
        "              {\n",
        "                \"token\": \"?\",\n",
        "                \"logprob\": -0.0008108172,\n",
        "                \"bytes\": [63]\n",
        "              },\n",
        "              {\n",
        "                \"token\": \"?\\n\",\n",
        "                \"logprob\": -7.184561,\n",
        "                \"bytes\": [63, 10]\n",
        "              }\n",
        "            ]\n",
        "          }\n",
        "        ]\n",
        "      },\n",
        "      \"finish_reason\": \"stop\"\n",
        "    }\n",
        "  ],\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 9,\n",
        "    \"completion_tokens\": 9,\n",
        "    \"total_tokens\": 18,\n",
        "    \"completion_tokens_details\": {\n",
        "      \"reasoning_tokens\": 0,\n",
        "      \"accepted_prediction_tokens\": 0,\n",
        "      \"rejected_prediction_tokens\": 0\n",
        "    }\n",
        "  },\n",
        "  \"system_fingerprint\": null\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "W0xBp1HfRTRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPENAI Embedding\n",
        "\n",
        "The **Create Embeddings API** generates an embedding vector representing the input text. Embeddings are numerical vector representations of text that can be used for various machine learning and search tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## Endpoint\n",
        "```\n",
        "POST https://api.openai.com/v1/embeddings\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Request Body\n",
        "\n",
        "### Parameters\n",
        "\n",
        "| Name               | Type             | Required | Description                                                                                                                                  |\n",
        "|--------------------|------------------|----------|----------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **input**          | string or array  | Required | Input text to embed, encoded as a string or array of tokens. The input must not exceed the model's token limit. Multiple inputs can be sent as an array. |\n",
        "| **model**          | string           | Required | ID of the model to use. For example, `text-embedding-ada-002`. Refer to the [List Models API](#) for available models.                       |\n",
        "| **encoding_format**| string           | Optional | Defaults to `float`. Format for returning embeddings. Can be `float` or `base64`.                                                           |\n",
        "| **dimensions**     | integer          | Optional | Number of dimensions for the output embeddings. Supported in `text-embedding-3` and later models.                                           |\n",
        "| **user**           | string           | Optional | A unique identifier representing the end-user. Useful for monitoring and abuse detection.                                                   |\n",
        "\n",
        "---\n",
        "\n",
        "## Returns\n",
        "\n",
        "- **A list of embedding objects**.\n",
        "\n",
        "Each embedding object contains the following:\n",
        "\n",
        "| Field           | Type    | Description                                                                                       |\n",
        "|------------------|---------|---------------------------------------------------------------------------------------------------|\n",
        "| **object**      | string  | Always `embedding`.                                                                              |\n",
        "| **embedding**   | array   | The embedding vector as a list of floats.                                                        |\n",
        "| **index**       | integer | The index of the embedding in the input array (for batch requests).                              |\n",
        "\n",
        "### Token Usage Statistics\n",
        "\n",
        "| Field             | Type    | Description                                                                                   |\n",
        "|-------------------|---------|-----------------------------------------------------------------------------------------------|\n",
        "| **prompt_tokens** | integer | Number of tokens in the input text.                                                           |\n",
        "| **total_tokens**  | integer | Total tokens used in the request.                                                             |\n",
        "\n",
        "---\n",
        "\n",
        "## Example cURL Request\n",
        "\n",
        "This example generates an embedding for the input text *\"The food was delicious and the waiter...\"* using the `text-embedding-ada-002` model.\n",
        "\n",
        "```bash\n",
        "curl https://api.openai.com/v1/embeddings \\\n",
        "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\n",
        "    \"input\": \"The food was delicious and the waiter...\",\n",
        "    \"model\": \"text-embedding-ada-002\",\n",
        "    \"encoding_format\": \"float\"\n",
        "  }'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Example Response\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"object\": \"list\",\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"object\": \"embedding\",\n",
        "      \"embedding\": [\n",
        "        0.0023064255,\n",
        "        -0.009327292,\n",
        "        // ...(1536 floats total for ada-002)\n",
        "        -0.0028842222\n",
        "      ],\n",
        "      \"index\": 0\n",
        "    }\n",
        "  ],\n",
        "  \"model\": \"text-embedding-ada-002\",\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 8,\n",
        "    \"total_tokens\": 8\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Notes\n",
        "\n",
        "1. **Embedding Dimensions**:\n",
        "   - The output vector has 1536 dimensions for `text-embedding-ada-002`.\n",
        "   - For models supporting configurable dimensions, use the `dimensions` parameter.\n",
        "\n",
        "2. **Batch Requests**:\n",
        "   - Multiple inputs can be provided as an array of strings. Each input will have its embedding object in the response.\n",
        "\n",
        "3. **Encoding Format**:\n",
        "   - Default format is `float` (list of floating-point numbers).\n",
        "   - Use `base64` for compressed representations.\n",
        "\n",
        "4. **Token Limits**:\n",
        "   - Input tokens must not exceed the model's limit. For `text-embedding-ada-002`, this is 8192 tokens.\n",
        "\n",
        "5. **User Identification**:\n",
        "   - The `user` parameter can be helpful for tracking usage or handling abuse scenarios."
      ],
      "metadata": {
        "id": "s798olxLRZGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MISTRAL Chat Completion\n",
        "\n",
        "The **Chat Completion API** generates conversational responses based on a sequence of input messages. It provides a wide range of customization options to control the behavior and creativity of the responses.\n",
        "\n",
        "---\n",
        "\n",
        "## Authorization\n",
        "\n",
        "**HTTP Authorization Scheme**: `bearer`  \n",
        "**Header**:  \n",
        "```\n",
        "Authorization: Bearer <API_KEY>\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Endpoint\n",
        "```\n",
        "POST https://api.mistral.ai/v1/chat/completions\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Request Body Schema\n",
        "\n",
        "The request body must be sent in **JSON format** with the following fields:\n",
        "\n",
        "### Required Parameters\n",
        "\n",
        "| Parameter        | Type                      | Description                                                                                                                  |\n",
        "|------------------|---------------------------|------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **model**        | string or null            | The model ID to use. Use the **List Available Models API** to view all models or refer to the model overview documentation.   |\n",
        "| **messages**     | array of objects          | A list of input messages, each with a `role` and `content`. Represents the conversation history or single-turn input.         |\n",
        "\n",
        "### Optional Parameters\n",
        "\n",
        "| Parameter           | Type                    | Default  | Description                                                                                                                      |\n",
        "|---------------------|-------------------------|----------|----------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **temperature**     | number or null          | Varies   | Controls randomness. Recommended range: `0.0` (deterministic) to `0.7` (creative). Avoid altering with `top_p` simultaneously.  |\n",
        "| **top_p**           | number                  | 1        | Nucleus sampling. Considers tokens within the top `p` probability mass. Use one of `temperature` or `top_p`, not both.          |\n",
        "| **max_tokens**      | integer or null         | N/A      | Maximum tokens for the completion. Prompt tokens + `max_tokens` must not exceed the model's context length.                     |\n",
        "| **stream**          | boolean                 | false    | Whether to stream the response incrementally using **Server-Sent Events (SSEs)**.                                               |\n",
        "| **stop**            | string or array         | N/A      | Stop generation when the specified token(s) are encountered.                                                                    |\n",
        "| **random_seed**     | integer or null         | N/A      | A seed value for random sampling, ensuring deterministic outputs across calls when set.                                         |\n",
        "| **response_format** | object                  | `{}`     | The format for the response content.                                                                                            |\n",
        "| **tools**           | array of objects or null| null     | A list of tools the model may invoke during the conversation.                                                                   |\n",
        "| **tool_choice**     | string                  | \"auto\"   | Specifies how the model should use tools: `\"auto\"`, `\"any\"`, or a specific tool.                                                |\n",
        "| **presence_penalty**| number                  | 0        | Penalizes the model for repeating words or phrases, encouraging diversity. Range: `-2.0` to `2.0`.                              |\n",
        "| **frequency_penalty**| number                 | 0        | Penalizes the model for frequently repeating words based on frequency. Range: `-2.0` to `2.0`.                                   |\n",
        "| **n**               | integer or null         | 1        | Number of completions to return. Input tokens are only billed once.                                                             |\n",
        "| **safe_prompt**     | boolean                 | false    | Injects a safety prompt at the start of the conversation to ensure a safer response.                                            |\n",
        "\n",
        "---\n",
        "\n",
        "## Example Request\n",
        "\n",
        "### Non-Streaming Request\n",
        "\n",
        "```bash\n",
        "curl --location \"https://api.mistral.ai/v1/chat/completions\" \\\n",
        "     --header 'Content-Type: application/json' \\\n",
        "     --header 'Accept: application/json' \\\n",
        "     --header \"Authorization: Bearer $MISTRAL_API_KEY\" \\\n",
        "     --data '{\n",
        "        \"model\": \"mistral-large-latest\",\n",
        "        \"messages\": [\n",
        "          {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Who is the best French painter? Answer in one short sentence.\"\n",
        "          }\n",
        "        ],\n",
        "        \"temperature\": 0.5,\n",
        "        \"top_p\": 1,\n",
        "        \"max_tokens\": 50,\n",
        "        \"stream\": false,\n",
        "        \"presence_penalty\": 0,\n",
        "        \"frequency_penalty\": 0\n",
        "      }'\n",
        "```\n",
        "\n",
        "### Streaming Request\n",
        "\n",
        "```bash\n",
        "curl --location \"https://api.mistral.ai/v1/chat/completions\" \\\n",
        "     --header 'Content-Type: application/json' \\\n",
        "     --header 'Accept: application/json' \\\n",
        "     --header \"Authorization: Bearer $MISTRAL_API_KEY\" \\\n",
        "     --data '{\n",
        "        \"model\": \"mistral-large-latest\",\n",
        "        \"messages\": [\n",
        "          {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What is the best French cheese?\"\n",
        "          }\n",
        "        ],\n",
        "        \"temperature\": 0.7,\n",
        "        \"top_p\": 1,\n",
        "        \"max_tokens\": 100,\n",
        "        \"stream\": true\n",
        "      }'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Example Responses\n",
        "\n",
        "### Non-Streaming Response\n",
        "```json\n",
        "{\n",
        "  \"id\": \"cmpl-e5cc70bb28c444948073e77776eb30ef\",\n",
        "  \"object\": \"chat.completion\",\n",
        "  \"model\": \"mistral-large-latest\",\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 16,\n",
        "    \"completion_tokens\": 34,\n",
        "    \"total_tokens\": 50\n",
        "  },\n",
        "  \"created\": 1702256327,\n",
        "  \"choices\": [\n",
        "    {\n",
        "      \"index\": 0,\n",
        "      \"message\": {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"The best French painter is Claude Monet, a pioneer of Impressionism.\",\n",
        "        \"tool_calls\": {}\n",
        "      },\n",
        "      \"finish_reason\": \"stop\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "### Streaming Response\n",
        "```plaintext\n",
        "event: message_start\n",
        "data: {\"type\": \"message_start\", \"message\": {\"id\": \"msg_1nZdL29xx5MUA1yADyHTEsnR8uuvGzszyY\", \"type\": \"message\", \"role\": \"assistant\", \"content\": [], \"model\": \"mistral-large-latest\", \"stop_reason\": null, \"stop_sequence\": null, \"usage\": {\"input_tokens\": 25, \"output_tokens\": 1}}}\n",
        "\n",
        "event: content_block_delta\n",
        "data: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"text_delta\", \"text\": \"Hello\"}}\n",
        "\n",
        "event: message_delta\n",
        "data: {\"type\": \"message_delta\", \"delta\": {\"stop_reason\": \"end_turn\"}, \"usage\": {\"output_tokens\": 15}}\n",
        "\n",
        "event: message_stop\n",
        "data: {\"type\": \"message_stop\"}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Responses\n",
        "\n",
        "### Success Response (200)\n",
        "| Field              | Type          | Description                                                                 |\n",
        "|--------------------|---------------|-----------------------------------------------------------------------------|\n",
        "| **id**             | string        | Unique identifier for the chat completion.                                 |\n",
        "| **object**         | string        | Always `\"chat.completion\"`.                                                |\n",
        "| **model**          | string        | The model used to generate the response.                                   |\n",
        "| **usage**          | object        | Token usage details: `prompt_tokens`, `completion_tokens`, and `total_tokens`. |\n",
        "| **created**        | integer       | Unix timestamp of when the response was generated.                         |\n",
        "| **choices**        | array         | List of completions, each containing a `message`.                          |\n",
        "\n",
        "### Error Response (422)\n",
        "If validation fails, the API will return an error response.\n",
        "\n",
        "| Field              | Type          | Description                                                                 |\n",
        "|--------------------|---------------|-----------------------------------------------------------------------------|\n",
        "| **type**           | string        | The type of error (e.g., `\"validation_error\"`).                             |\n",
        "| **message**        | string        | A detailed error message.                                                  |\n",
        "\n",
        "**Example:**\n",
        "```json\n",
        "{\n",
        "  \"type\": \"validation_error\",\n",
        "  \"message\": \"Invalid model ID.\"\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Notes\n",
        "\n",
        "1. **Model Compatibility**: Ensure that the `model` supports the desired parameters (e.g., `temperature`, `streaming`).\n",
        "2. **Token Limits**: Total tokens (`prompt_tokens` + `max_tokens`) must not exceed the model's context length.\n",
        "3. **Stop Tokens**: Use the `stop` parameter to define custom stop sequences for finer control.\n",
        "4. **Randomness**: Adjust `temperature` or `top_p` to modify the creativity and determinism of the output.\n",
        "5. **Streaming**: Enable `stream: true` for real-time token delivery.\n"
      ],
      "metadata": {
        "id": "h71u7TVSRbpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# MISTRAL Embedding\n",
        "\n",
        "The **Embeddings API** generates vector representations of input text that can be used in downstream tasks such as similarity comparisons, clustering, or search indexing.\n",
        "\n",
        "---\n",
        "\n",
        "## Authorization\n",
        "\n",
        "**HTTP Authorization Scheme**: `bearer`  \n",
        "**Header**:  \n",
        "```\n",
        "Authorization: Bearer <API_KEY>\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Endpoint\n",
        "```\n",
        "POST https://api.mistral.ai/v1/embeddings\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Request Body Schema\n",
        "\n",
        "The request body must be sent in **JSON format** with the following fields:\n",
        "\n",
        "### Required Parameters\n",
        "\n",
        "| Parameter           | Type                     | Description                                                      |\n",
        "|---------------------|--------------------------|------------------------------------------------------------------|\n",
        "| **input**           | string or array of strings | The text or list of texts to embed.                             |\n",
        "| **model**           | string                   | The ID of the embedding model to use. Default: `\"mistral-embed\"`. |\n",
        "\n",
        "### Optional Parameters\n",
        "\n",
        "| Parameter           | Type                     | Default       | Description                                                      |\n",
        "|---------------------|--------------------------|---------------|------------------------------------------------------------------|\n",
        "| **encoding_format** | string or null           | `\"float\"`     | The format for the returned embeddings. Use `\"float\"` for numerical vectors. |\n",
        "\n",
        "---\n",
        "\n",
        "## Example Request\n",
        "\n",
        "### Single Input\n",
        "```bash\n",
        "curl --location \"https://api.mistral.ai/v1/embeddings\" \\\n",
        "     --header \"Content-Type: application/json\" \\\n",
        "     --header \"Authorization: Bearer $MISTRAL_API_KEY\" \\\n",
        "     --data '{\n",
        "        \"input\": \"Embed this sentence.\",\n",
        "        \"model\": \"mistral-embed\",\n",
        "        \"encoding_format\": \"float\"\n",
        "      }'\n",
        "```\n",
        "\n",
        "### Batch Input\n",
        "```bash\n",
        "curl --location \"https://api.mistral.ai/v1/embeddings\" \\\n",
        "     --header \"Content-Type: application/json\" \\\n",
        "     --header \"Authorization: Bearer $MISTRAL_API_KEY\" \\\n",
        "     --data '{\n",
        "        \"input\": [\n",
        "          \"Embed this sentence.\",\n",
        "          \"As well as this one.\"\n",
        "        ],\n",
        "        \"model\": \"mistral-embed\",\n",
        "        \"encoding_format\": \"float\"\n",
        "      }'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Responses\n",
        "\n",
        "### Success Response (200)\n",
        "\n",
        "| Field              | Type          | Description                                                                 |\n",
        "|--------------------|---------------|-----------------------------------------------------------------------------|\n",
        "| **id**             | string        | Unique identifier for the embedding request.                               |\n",
        "| **object**         | string        | Always `\"embedding\"`.                                                      |\n",
        "| **model**          | string        | The model used to generate the embeddings.                                 |\n",
        "| **usage**          | object        | Token usage details, including `prompt_tokens`, `completion_tokens`, and `total_tokens`. |\n",
        "| **data**           | array         | A list of embeddings. Each embedding contains an object, an array of numerical vectors, and an index. |\n",
        "\n",
        "#### Example Success Response\n",
        "```json\n",
        "{\n",
        "  \"id\": \"cmpl-e5cc70bb28c444948073e77776eb30ef\",\n",
        "  \"object\": \"embedding\",\n",
        "  \"model\": \"mistral-embed\",\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 16,\n",
        "    \"completion_tokens\": 34,\n",
        "    \"total_tokens\": 50\n",
        "  },\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"object\": \"embedding\",\n",
        "      \"embedding\": [0.1, 0.2, 0.3],\n",
        "      \"index\": 0\n",
        "    },\n",
        "    {\n",
        "      \"object\": \"embedding\",\n",
        "      \"embedding\": [0.4, 0.5, 0.6],\n",
        "      \"index\": 1\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "### Error Response (422)\n",
        "\n",
        "If validation fails, the API will return an error response.\n",
        "\n",
        "| Field              | Type          | Description                                                                 |\n",
        "|--------------------|---------------|-----------------------------------------------------------------------------|\n",
        "| **detail**         | array         | List of error details.                                                     |\n",
        "| **loc**            | array         | The location of the error.                                                 |\n",
        "| **msg**            | string        | A detailed error message.                                                  |\n",
        "| **type**           | string        | The type of error.                                                         |\n",
        "\n",
        "#### Example Error Response\n",
        "```json\n",
        "{\n",
        "  \"detail\": [\n",
        "    {\n",
        "      \"loc\": [\"body\", \"input\"],\n",
        "      \"msg\": \"Invalid input text format.\",\n",
        "      \"type\": \"validation_error\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Notes\n",
        "\n",
        "1. **Batch Processing**: The API supports multiple inputs in a single request. Each input is embedded separately, and the results are returned in the `data` array, indexed by their order in the input array.\n",
        "2. **Embedding Format**: The embeddings are numerical vectors returned in the `encoding_format` specified (default is `\"float\"`).\n",
        "3. **Model Selection**: Use the `\"mistral-embed\"` model for standard embeddings. Additional models may be available for specialized tasks.\n",
        "4. **Token Usage**: The `usage` field provides information about token consumption for billing and optimization purposes.\n",
        "5. **Error Handling**: Ensure inputs are properly formatted as strings or arrays of strings to avoid validation errors.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zZpIAjzzRe0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COHERE Chat Completion\n",
        "\n",
        "The Cohere Chat API allows you to generate text responses to user messages, supporting tools and system messages for advanced conversation flows. This endpoint is especially useful for creating conversational AI applications.\n",
        "\n",
        "## Endpoint\n",
        "**POST** `https://api.cohere.com/v2/chat`\n",
        "\n",
        "### Purpose\n",
        "Generates a text response to a user message, optionally streaming the response token by token.\n",
        "\n",
        "---\n",
        "\n",
        "## Headers\n",
        "\n",
        "| Header Name         | Type   | Required | Description                              |\n",
        "|---------------------|--------|----------|------------------------------------------|\n",
        "| `X-Client-Name`     | String | Optional | Name of the project making the request. |\n",
        "| `Authorization`     | String | Required | Bearer token for authentication.        |\n",
        "| `accept`            | String | Required | Must be set to `application/json`.       |\n",
        "| `content-type`      | String | Required | Must be set to `application/json`.       |\n",
        "\n",
        "---\n",
        "\n",
        "## Request\n",
        "\n",
        "The request body expects a JSON object with the following structure:\n",
        "\n",
        "### Required Fields\n",
        "\n",
        "| Field           | Type   | Required | Description                                                                                     |\n",
        "|------------------|--------|----------|-------------------------------------------------------------------------------------------------|\n",
        "| `model`         | String | Yes      | Name of the model to use (e.g., `command-r` or `command-r-plus`).                              |\n",
        "| `messages`      | Array  | Yes      | List of messages representing the conversation. See \"Message Structure\" below for details.     |\n",
        "\n",
        "### Optional Fields\n",
        "\n",
        "| Field               | Type    | Description                                                                                                         |\n",
        "|---------------------|---------|---------------------------------------------------------------------------------------------------------------------|\n",
        "| `stream`            | Boolean | If true, streams the response token by token.                                                                      |\n",
        "| `tools`             | Array   | List of available tools (functions) the model can suggest invoking.                                                |\n",
        "| `response_format`   | Object  | Ensures a specific output format, like JSON. Can include a JSON schema.                                            |\n",
        "| `safety_mode`       | Enum    | Options: `CONTEXTUAL`, `STRICT`, `OFF`. Default: `CONTEXTUAL`. Determines the safety level for generated content.   |\n",
        "| `max_tokens`        | Integer | Limits the number of tokens in the response.                                                                       |\n",
        "| `temperature`       | Float   | Controls response randomness. Default: `0.3`. Lower = more deterministic, higher = more random.                   |\n",
        "| `frequency_penalty` | Float   | Penalizes repetition of tokens. Range: 0.0 to 1.0.                                                                 |\n",
        "| `presence_penalty`  | Float   | Penalizes the presence of repeated tokens. Range: 0.0 to 1.0.                                                      |\n",
        "| `k`                 | Float   | Considers the top-k most likely tokens at each step. Range: 0 to 500.                                              |\n",
        "| `p`                 | Float   | Ensures only tokens with total probability `p` are considered. Range: 0.01 to 0.99.                                |\n",
        "\n",
        "---\n",
        "\n",
        "### Message Structure\n",
        "\n",
        "Messages must include a `role` and `content`. Roles can be `user`, `assistant`, `system`, or `tool`. Content varies by role.\n",
        "\n",
        "#### User Message\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"role\": \"user\",\n",
        "  \"content\": {\n",
        "    \"type\": \"text\",\n",
        "    \"text\": \"Hello world!\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### Assistant Message\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"role\": \"assistant\",\n",
        "  \"content\": [\n",
        "    {\n",
        "      \"type\": \"text\",\n",
        "      \"text\": \"Hello! How can I help you today?\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "#### System Message\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"role\": \"system\",\n",
        "  \"content\": {\n",
        "    \"type\": \"text\",\n",
        "    \"text\": \"You are a helpful assistant.\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### Tool Message\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"role\": \"tool\",\n",
        "  \"content\": {\n",
        "    \"type\": \"text\",\n",
        "    \"text\": \"{\\\"action\\\": \\\"search\\\", \\\"query\\\": \\\"weather today\\\"}\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Response\n",
        "\n",
        "The response returns a JSON object with the following fields:\n",
        "\n",
        "| Field            | Type   | Description                                                                                      |\n",
        "|-------------------|--------|--------------------------------------------------------------------------------------------------|\n",
        "| `id`             | String | Unique identifier for the response.                                                             |\n",
        "| `finish_reason`  | Enum   | Reason for finishing the response. Possible values: `COMPLETE`, `MAX_TOKENS`, `STOP_SEQUENCE`.  |\n",
        "| `message`        | Object | The generated message from the assistant.                                                       |\n",
        "| `usage`          | Object | Details about token usage and billing.                                                          |\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Example cURL Command\n",
        "\n",
        "```bash\n",
        "curl --request POST \\\n",
        "  --url https://api.cohere.com/v2/chat \\\n",
        "  --header 'accept: application/json' \\\n",
        "  --header 'content-type: application/json' \\\n",
        "  --header \"Authorization: Bearer $CO_API_KEY\" \\\n",
        "  --data '{\n",
        "    \"model\": \"command-r-plus-08-2024\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"Hello world!\"\n",
        "        }\n",
        "      }\n",
        "    ]\n",
        "  }'\n",
        "```\n",
        "\n",
        "### Example Response\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"id\": \"c14c80c3-18eb-4519-9460-6c92edd8cfb4\",\n",
        "  \"finish_reason\": \"COMPLETE\",\n",
        "  \"message\": {\n",
        "    \"role\": \"assistant\",\n",
        "    \"content\": [\n",
        "      {\n",
        "        \"type\": \"text\",\n",
        "        \"text\": \"Hello! How can I assist you today?\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  \"usage\": {\n",
        "    \"billed_units\": {\n",
        "      \"input_tokens\": 5,\n",
        "      \"output_tokens\": 418\n",
        "    },\n",
        "    \"tokens\": {\n",
        "      \"input_tokens\": 71,\n",
        "      \"output_tokens\": 418\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "# Documentation for Chat API with Streaming (POST)\n",
        "\n",
        "The **Chat API with Streaming** endpoint allows you to generate text responses to user messages in real time, token by token. This feature is ideal for building interactive and responsive applications.\n",
        "\n",
        "---\n",
        "\n",
        "## Endpoint\n",
        "\n",
        "**POST** `https://api.cohere.com/v2/chat`\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "Generate a streamed token-by-token response to user input, providing real-time conversational feedback.\n",
        "\n",
        "---\n",
        "\n",
        "## Headers\n",
        "\n",
        "| Header Name         | Type   | Required | Description                              |\n",
        "|---------------------|--------|----------|------------------------------------------|\n",
        "| `Authorization`     | String | Required | Bearer token for authentication.        |\n",
        "| `accept`            | String | Required | Must be set to `application/json`.       |\n",
        "| `content-type`      | String | Required | Must be set to `application/json`.       |\n",
        "| `X-Client-Name`     | String | Optional | Name of the project making the request. |\n",
        "\n",
        "---\n",
        "\n",
        "## Request Body\n",
        "\n",
        "The request expects a JSON object with the following structure:\n",
        "\n",
        "### Required Fields\n",
        "\n",
        "| Field           | Type   | Description                                                                                     |\n",
        "|------------------|--------|-------------------------------------------------------------------------------------------------|\n",
        "| `stream`        | Boolean | Set to `true` to enable streaming responses.                                                   |\n",
        "| `model`         | String | Name of the Cohere model to use, such as `command-r-plus-08-2024`.                              |\n",
        "| `messages`      | Array  | List of messages representing the conversation. See \"Message Structure\" below.                 |\n",
        "\n",
        "### Optional Fields\n",
        "\n",
        "| Field               | Type    | Description                                                                                                         |\n",
        "|---------------------|---------|---------------------------------------------------------------------------------------------------------------------|\n",
        "| `tools`             | Array   | List of tools (functions) the model can invoke.                                                                    |\n",
        "| `response_format`   | Object  | Specifies the desired output format, including optional JSON schema for structure enforcement.                     |\n",
        "| `safety_mode`       | Enum    | Options: `CONTEXTUAL`, `STRICT`, `OFF`. Default: `CONTEXTUAL`. Controls safety level for content generation.        |\n",
        "| `max_tokens`        | Integer | Limits the maximum number of tokens generated in the response.                                                     |\n",
        "| `temperature`       | Float   | Controls randomness in the output. Default: `0.3`. Lower = deterministic, higher = more creative.                  |\n",
        "| `frequency_penalty` | Float   | Penalizes token repetition. Range: 0.0 to 1.0.                                                                     |\n",
        "| `presence_penalty`  | Float   | Penalizes repeated token presence. Range: 0.0 to 1.0.                                                              |\n",
        "| `k`                 | Float   | Restricts output to the top-k most likely tokens. Range: 0 to 500.                                                 |\n",
        "| `p`                 | Float   | Restricts output to tokens within a cumulative probability `p`. Range: 0.01 to 0.99.                               |\n",
        "\n",
        "---\n",
        "\n",
        "### Message Structure\n",
        "\n",
        "Messages must include a `role` and `content`. Supported roles are:\n",
        "- **user**: The input message from the user.\n",
        "- **assistant**: The model's response.\n",
        "- **system**: Instructions or configurations for the assistant.\n",
        "- **tool**: Information related to a tool invocation.\n",
        "\n",
        "#### Example Message Formats\n",
        "\n",
        "**User Message:**\n",
        "```json\n",
        "{\n",
        "  \"role\": \"user\",\n",
        "  \"content\": {\n",
        "    \"type\": \"text\",\n",
        "    \"text\": \"Hello world!\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "**Assistant Message:**\n",
        "```json\n",
        "{\n",
        "  \"role\": \"assistant\",\n",
        "  \"content\": [\n",
        "    {\n",
        "      \"type\": \"text\",\n",
        "      \"text\": \"Hello! How can I assist you today?\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "**System Message:**\n",
        "```json\n",
        "{\n",
        "  \"role\": \"system\",\n",
        "  \"content\": {\n",
        "    \"type\": \"text\",\n",
        "    \"text\": \"You are a helpful assistant.\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Server-Sent Events (Streaming)\n",
        "\n",
        "When `stream` is set to `true`, the response is delivered as **Server-Sent Events (SSE)**, enabling real-time delivery of tokens.\n",
        "\n",
        "### Event Types\n",
        "\n",
        "| Event Type         | Description                                                                                   |\n",
        "|---------------------|-----------------------------------------------------------------------------------------------|\n",
        "| `message-start`    | Indicates the start of the assistant's response message.                                      |\n",
        "| `content-start`    | Signals the beginning of a new content block.                                                 |\n",
        "| `content-delta`    | Contains a chunk of text (token). Multiple deltas make up the full message.                   |\n",
        "| `content-end`      | Indicates the end of a content block.                                                         |\n",
        "| `message-end`      | Marks the end of the response stream, including usage statistics and finish reason.            |\n",
        "\n",
        "---\n",
        "\n",
        "### Example Event Stream\n",
        "\n",
        "\n",
        "### Example cURL Command\n",
        "\n",
        "```bash\n",
        "curl --request POST \\\n",
        "  --url https://api.cohere.com/v2/chat \\\n",
        "  --header 'accept: application/json' \\\n",
        "  --header 'content-type: application/json' \\\n",
        "  --header \"Authorization: Bearer $CO_API_KEY\" \\\n",
        "  --data '{\n",
        "    \"stream\": true,\n",
        "    \"model\": \"command-r-plus-08-2024\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": {\n",
        "          \"type\": \"text\",\n",
        "          \"text\": \"Hello world!\"\n",
        "        }\n",
        "      }\n",
        "    ]\n",
        "  }'\n",
        "```\n",
        "---\n",
        "#### Start of Message\n",
        "```json\n",
        "{\n",
        "  \"type\": \"message-start\",\n",
        "  \"id\": \"cc5336e7-24f3-492d-a87c-d473907feb2c\",\n",
        "  \"delta\": {\n",
        "    \"message\": {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": [],\n",
        "      \"tool_plan\": \"\",\n",
        "      \"tool_calls\": [],\n",
        "      \"citations\": []\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### Content Start\n",
        "```json\n",
        "{\n",
        "  \"type\": \"content-start\",\n",
        "  \"index\": 0,\n",
        "  \"delta\": {\n",
        "    \"message\": {\n",
        "      \"content\": {\n",
        "        \"type\": \"text\",\n",
        "        \"text\": \"\"\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "#### Content Delta (Token by Token)\n",
        "```json\n",
        "{\"type\":\"content-delta\",\"index\":0,\"delta\":{\"message\":{\"content\":{\"text\":\"Hello\"}}}}\n",
        "{\"type\":\"content-delta\",\"index\":0,\"delta\":{\"message\":{\"content\":{\"text\":\"!\"}}}}\n",
        "{\"type\":\"content-delta\",\"index\":0,\"delta\":{\"message\":{\"content\":{\"text\":\" How\"}}}}\n",
        "{\"type\":\"content-delta\",\"index\":0,\"delta\":{\"message\":{\"content\":{\"text\":\" can\"}}}}\n",
        "{\"type\":\"content-delta\",\"index\":0,\"delta\":{\"message\":{\"content\":{\"text\":\" I\"}}}}\n",
        "{\"type\":\"content-delta\",\"index\":0,\"delta\":{\"message\":{\"content\":{\"text\":\" help\"}}}}\n",
        "{\"type\":\"content-delta\",\"index\":0,\"delta\":{\"message\":{\"content\":{\"text\":\" you\"}}}}\n",
        "{\"type\":\"content-delta\",\"index\":0,\"delta\":{\"message\":{\"content\":{\"text\":\" today\"}}}}\n",
        "{\"type\":\"content-delta\",\"index\":0,\"delta\":{\"message\":{\"content\":{\"text\":\"?\"}}}}\n",
        "```\n",
        "\n",
        "#### End of Content\n",
        "```json\n",
        "{\n",
        "  \"type\": \"content-end\",\n",
        "  \"index\": 0\n",
        "}\n",
        "```\n",
        "\n",
        "#### End of Message\n",
        "```json\n",
        "{\n",
        "  \"type\": \"message-end\",\n",
        "  \"delta\": {\n",
        "    \"finish_reason\": \"COMPLETE\",\n",
        "    \"usage\": {\n",
        "      \"billed_units\": {\n",
        "        \"input_tokens\": 3,\n",
        "        \"output_tokens\": 9\n",
        "      },\n",
        "      \"tokens\": {\n",
        "        \"input_tokens\": 209,\n",
        "        \"output_tokens\": 9\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "81IhXYoYRhx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# MISTRAL Embedding\n",
        "\n",
        "The **Embeddings API** generates vector representations of input text that can be used in downstream tasks such as similarity comparisons, clustering, or search indexing.\n",
        "\n",
        "---\n",
        "\n",
        "## Authorization\n",
        "\n",
        "**HTTP Authorization Scheme**: `bearer`  \n",
        "**Header**:  \n",
        "```\n",
        "Authorization: Bearer <API_KEY>\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Endpoint\n",
        "```\n",
        "POST https://api.mistral.ai/v1/embeddings\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Request Body Schema\n",
        "\n",
        "The request body must be sent in **JSON format** with the following fields:\n",
        "\n",
        "### Required Parameters\n",
        "\n",
        "| Parameter           | Type                     | Description                                                      |\n",
        "|---------------------|--------------------------|------------------------------------------------------------------|\n",
        "| **input**           | string or array of strings | The text or list of texts to embed.                             |\n",
        "| **model**           | string                   | The ID of the embedding model to use. Default: `\"mistral-embed\"`. |\n",
        "\n",
        "### Optional Parameters\n",
        "\n",
        "| Parameter           | Type                     | Default       | Description                                                      |\n",
        "|---------------------|--------------------------|---------------|------------------------------------------------------------------|\n",
        "| **encoding_format** | string or null           | `\"float\"`     | The format for the returned embeddings. Use `\"float\"` for numerical vectors. |\n",
        "\n",
        "---\n",
        "\n",
        "## Example Request\n",
        "\n",
        "### Single Input\n",
        "```bash\n",
        "curl --location \"https://api.mistral.ai/v1/embeddings\" \\\n",
        "     --header \"Content-Type: application/json\" \\\n",
        "     --header \"Authorization: Bearer $MISTRAL_API_KEY\" \\\n",
        "     --data '{\n",
        "        \"input\": \"Embed this sentence.\",\n",
        "        \"model\": \"mistral-embed\",\n",
        "        \"encoding_format\": \"float\"\n",
        "      }'\n",
        "```\n",
        "\n",
        "### Batch Input\n",
        "```bash\n",
        "curl --location \"https://api.mistral.ai/v1/embeddings\" \\\n",
        "     --header \"Content-Type: application/json\" \\\n",
        "     --header \"Authorization: Bearer $MISTRAL_API_KEY\" \\\n",
        "     --data '{\n",
        "        \"input\": [\n",
        "          \"Embed this sentence.\",\n",
        "          \"As well as this one.\"\n",
        "        ],\n",
        "        \"model\": \"mistral-embed\",\n",
        "        \"encoding_format\": \"float\"\n",
        "      }'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Responses\n",
        "\n",
        "### Success Response (200)\n",
        "\n",
        "| Field              | Type          | Description                                                                 |\n",
        "|--------------------|---------------|-----------------------------------------------------------------------------|\n",
        "| **id**             | string        | Unique identifier for the embedding request.                               |\n",
        "| **object**         | string        | Always `\"embedding\"`.                                                      |\n",
        "| **model**          | string        | The model used to generate the embeddings.                                 |\n",
        "| **usage**          | object        | Token usage details, including `prompt_tokens`, `completion_tokens`, and `total_tokens`. |\n",
        "| **data**           | array         | A list of embeddings. Each embedding contains an object, an array of numerical vectors, and an index. |\n",
        "\n",
        "#### Example Success Response\n",
        "```json\n",
        "{\n",
        "  \"id\": \"cmpl-e5cc70bb28c444948073e77776eb30ef\",\n",
        "  \"object\": \"embedding\",\n",
        "  \"model\": \"mistral-embed\",\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 16,\n",
        "    \"completion_tokens\": 34,\n",
        "    \"total_tokens\": 50\n",
        "  },\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"object\": \"embedding\",\n",
        "      \"embedding\": [0.1, 0.2, 0.3],\n",
        "      \"index\": 0\n",
        "    },\n",
        "    {\n",
        "      \"object\": \"embedding\",\n",
        "      \"embedding\": [0.4, 0.5, 0.6],\n",
        "      \"index\": 1\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "### Error Response (422)\n",
        "\n",
        "If validation fails, the API will return an error response.\n",
        "\n",
        "| Field              | Type          | Description                                                                 |\n",
        "|--------------------|---------------|-----------------------------------------------------------------------------|\n",
        "| **detail**         | array         | List of error details.                                                     |\n",
        "| **loc**            | array         | The location of the error.                                                 |\n",
        "| **msg**            | string        | A detailed error message.                                                  |\n",
        "| **type**           | string        | The type of error.                                                         |\n",
        "\n",
        "#### Example Error Response\n",
        "```json\n",
        "{\n",
        "  \"detail\": [\n",
        "    {\n",
        "      \"loc\": [\"body\", \"input\"],\n",
        "      \"msg\": \"Invalid input text format.\",\n",
        "      \"type\": \"validation_error\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Notes\n",
        "\n",
        "1. **Batch Processing**: The API supports multiple inputs in a single request. Each input is embedded separately, and the results are returned in the `data` array, indexed by their order in the input array.\n",
        "2. **Embedding Format**: The embeddings are numerical vectors returned in the `encoding_format` specified (default is `\"float\"`).\n",
        "3. **Model Selection**: Use the `\"mistral-embed\"` model for standard embeddings. Additional models may be available for specialized tasks.\n",
        "4. **Token Usage**: The `usage` field provides information about token consumption for billing and optimization purposes.\n",
        "5. **Error Handling**: Ensure inputs are properly formatted as strings or arrays of strings to avoid validation errors.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TQFk591TRqt8"
      }
    }
  ]
}